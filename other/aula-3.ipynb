{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Case-Startup-Health-Tech\" data-toc-modified-id=\"Case-Startup-Health-Tech-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Case Startup Health Tech</a></span></li><li><span><a href=\"#O-dataset\" data-toc-modified-id=\"O-dataset-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>O dataset</a></span></li><li><span><a href=\"#Escolhendo-as-Ferramentas\" data-toc-modified-id=\"Escolhendo-as-Ferramentas-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Escolhendo as Ferramentas</a></span><ul class=\"toc-item\"><li><span><a href=\"#NumPy\" data-toc-modified-id=\"NumPy-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>NumPy</a></span></li><li><span><a href=\"#pandas\" data-toc-modified-id=\"pandas-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>pandas</a></span></li><li><span><a href=\"#scikit-learn\" data-toc-modified-id=\"scikit-learn-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>scikit-learn</a></span></li></ul></li><li><span><a href=\"#O-Problema\" data-toc-modified-id=\"O-Problema-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>O Problema</a></span></li><li><span><a href=\"#Conceitos-Básicos-de-Machine-Learning\" data-toc-modified-id=\"Conceitos-Básicos-de-Machine-Learning-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Conceitos Básicos de Machine Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Dataset\" data-toc-modified-id=\"Dataset-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Dataset</a></span></li><li><span><a href=\"#Samples\" data-toc-modified-id=\"Samples-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Samples</a></span></li><li><span><a href=\"#Design-Matrix-X\" data-toc-modified-id=\"Design-Matrix-X-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Design Matrix X</a></span></li><li><span><a href=\"#Target-Values-y\" data-toc-modified-id=\"Target-Values-y-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Target Values y</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#IMPORTANTE\" data-toc-modified-id=\"IMPORTANTE-5.4.0.1\"><span class=\"toc-item-num\">5.4.0.1&nbsp;&nbsp;</span><font color=\"red\">IMPORTANTE</font></a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Test-Split\" data-toc-modified-id=\"Train-Test-Split-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Train Test Split</a></span></li><li><span><a href=\"#Cross-Validation\" data-toc-modified-id=\"Cross-Validation-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Cross-Validation</a></span></li></ul></li><li><span><a href=\"#Fluxograma-do-Baseline-Model\" data-toc-modified-id=\"Fluxograma-do-Baseline-Model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Fluxograma do Baseline Model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Startup Health Tech \n",
    "\n",
    "Estamos trabalhando como consultores para uma ONG. Eles querem criar um aparelho que recebe dados do usuário e devolve uma previsão binária 1, caso ele tenha diabetes, ou 0 caso ele não tenha.<br>\n",
    "\n",
    "Nosso primeiro passo como consultores é perguntar se há alguma base de dados que contenha variáveis que os especialistas acreditam ser boas features para fazer essa previsão. Estas features devem estar ligadas com uma **flag** binária 1(indicando que aquele paciente tem diabetes) e 0(indicando que ele não tem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O dataset\n",
    "Felizmente eles possuem um conjunto de dados que poderemos usar em nosso **MODELO**. Há algumas coisas que precisamos saber sobre ele:\n",
    "- Todos os paciente são mulheres\n",
    "- Todos os paciente têm mais de 21 anos\n",
    "- Todos os pacientes pertencem à etnia Pima de nativos americanos da região do Arizona\n",
    "\n",
    "Obviamente que isso poderia gerar algumas restrições, podemos generalizar isso para homens? Teríamos os mesmos resultados para outras etnias?<br>\n",
    "\n",
    "Há 8 features que foram previamente escolhidas pelos cientistas da *ONG*, são elas:\n",
    "- Pregnancies: número de vezes grávida\n",
    "- Glucose: concentração de glicose plasmática a 2 horas em um teste oral de tolerância à glicose\n",
    "- Blood Pressure: Pressão arterial diastólica (mm Hg)\n",
    "- Skin Thickness: Espessura da dobra da pele do tríceps (mm)\n",
    "- Insulin: Insulina de 2 horas (mu U / ml)\n",
    "- BMI: Índice de massa corporal (peso em kg / (altura em m) ^ 2)\n",
    "- Diabetes Pedigree Function: uma síntese da história do diabetes mellitus em parentes e a relação genética desses parentes com o sujeito\n",
    "- Age: idade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Escolhendo as Ferramentas \n",
    "A imagem abaixo mostra um esquema das ferramentas que mais usaremos no dia a dia. Existem muitas outras bibliotecas, mas acredite, caso você tenha muita intimidade com os *packages* NumPy, pandas e scikit-learn e com a linguagem Python, você será capaz de resolver quase todos os desafios que encontrar, bastará criatividade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stack Machine Learning](images/stack_inicial.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NumPy\n",
    "Não dá para colocar em palavras a importância desse *package*. Vou citar a própria documentação:\n",
    "> NumPy is the fundamental package for scientific computing in Python. It is a Python library that provides a multidimensional array object, various derived objects (such as masked arrays and matrices), and an assortment of routines for fast operations on arrays, including mathematical, logical, shape manipulation, sorting, selecting, I/O, discrete Fourier transforms, basic linear algebra, basic statistical operations, random simulation and much more\n",
    "\n",
    "Ela foi parte do projeto que permitiu o Event Horizon Telescope capturar a primeira imagem de um buraco negro e também foi usada pelos cientistas do LIGO que confirmou a existência de ondas gravitacionais. O pacote até mereceu um artigo na nature [Array programming with NumPy](https://www.nature.com/articles/s41586-020-2649-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas\n",
    "A biblioteca tão querida e largamente usada pela comunidade para manipular e analisar dados. Caso você use dados em formatos de tabelas, aquelas que aparecem no Excel da sua área ou em Banco de Dados, essa é a biblioteca que você estava procurando para sair do mundinho do Office. E convenhamos, é quase certo que a maioria dos dados da sua empresa estão nesses formatos..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scikit-learn\n",
    "Biblioteca construída em cima do NumPy, contém ferramentas para análise de dados preditiva. Com ela, podemos facilmente:\n",
    "- Treinar um algoritmo de Machine Learning complexo e usá-lo para fazer previsões\n",
    "- Usar a API da biblioteca para transformar e pré-processar os dados\n",
    "- Combinar transformações, pré-procesadores e algoritmos de Machine Learning tudo dentro de um único objeto chamado Pipeline\n",
    "- Avaliar nossos modelos\n",
    "- Fazer procuras automáticas para acharmos os melhores *hyper-parameters* de um ou mais algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Problema\n",
    "Temos que usar features para prever se uma dada paciente tem diabetes(1) ou não(0), temos dados históricos para isso. \n",
    "Esse é um problema de Machine Learning Supervisionado e uma tarefa de classifição, já que queremos colocar as pacientes em duas classes, que recebem nomes especiais:\n",
    "- 1: caso positivo, indica que a paciente tem diabetes\n",
    "- 0: caso negativo, indica que a paciente não tem diabetes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceitos Básicos de Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "O dataset é o conjunto dos dados que usaremos para alimentar um modelo completo no *scikit-learn*. Na maioria das vezes eles não chegam da maneira que iremos mostrar aqui, nosso trabalho é usar a biblioteca *pandas* e fazer essas manipulações, até encontrarmos o formato pedido.\n",
    "- DATASET: conjunto total dos dados em formato de tabela. Veja a imagem abaixo\n",
    "- Shape do DATASET: indica o número de elementos que temos em cada dimensão do dataset, no nosso exemplo hipotético, temos duas dimensões, logo, o shape ficariam (12, 3). Indicando que temos 12 linhas e 3 colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso acima, temos um problema de Machine Learning Supervisionado e uma tarefa de Classificação Binária, já que temos dois valores que queremos prever, zero e um.<br>\n",
    "\n",
    "Uma tarefa de classificação binária responde uma pergunta com sim ou não, no nosso caso:\n",
    "a paciente tem diabetes?\n",
    "- 1, caso tenha diabetes, chamamos também de caso positivo\n",
    "- 0, caso não tenha diabetes, também chamado de caso negativo\n",
    "\n",
    "Grave esses nomes, mais para frente eles serão importantes para usarmos métricas mais adequadas para a nossa tarefa de classificação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples\n",
    "São as linhas do DATASET, elas contêm o fenômeno que queremos prever, junto com os seus valores respectivos de features.\n",
    "Em destaque, temos a sample de número 1, sua primeira feature(Feat_1) tem valor 3 e sua segunda feature(Feat_2) tem valor 18. Essa *sample* não possui diabetes, como indicada pelo 0(zero) na coluna Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/sample_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Matrix X \n",
    "Conjunto das features do dataset, ela deve ter duas dimensões 2D(n_samples, n_features). Em destaque temos a Design Matrix X com as cinco primeiras samples do dataset. \n",
    "- *shape da Design Matrix X*: indica o número de elementos que temos em cada dimensão da Design Matrix X, no nosso caso, (12, 2), ou seja, temos 12 linhas e 2 colunas de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/design_matrix_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Values y\n",
    "É a coluna com os valores do fenômeno que queremos prever, ela geralmente está em 1D(n_samples,), mas pode ter mais dimensões, vamos falar sobre isso mais para frente. Em destaque temos os cinco primeiros valores do Target Values y\n",
    "- *shape* do Target Values y: indica o número de elementos que temos em cada dimensão, no nosso caso, (12,), ou seja, temos 12 samples em uma única dimensão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/target_val.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>IMPORTANTE</font>\n",
    "Veja que os shapes da Design Matrix X e do Target Values y, devem ter sempre o mesmo número de samples, no nosso caso,\n",
    "- Design Matrix X: (12, 2)\n",
    "- Target Values y: (12,)\n",
    "\n",
    "Isso significa que cada sample tem suas features e seu valor que queremos prever, assim podemos treinar o estimator do sciki-learn para ele achar uma relação entre X e y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "Para conseguirmos estimar como nosso modelo se sairá na vida real, usamos uma estratégia de separar aleatoriamente uma porcentagem de samples do dataset. Essa deve ser a primeira coisa que você deve fazer antes de transformar ou pré-processar seus dados, sério, na dúvida, essa é sempre a primeira coisa que você deve fazer, mesmo que para explorar graficamente seu dataset. Vai que você encontra algum \"padrão\" que só estava na sua cabeça, ou que só estava na parte dos dados que você viu, isso pode ser um problema. Vamos ver um passo a passo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Primeira coisa, separe seus dados entre Design Matrix X e Target Values y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/split_feat_label.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Segunda coisa, separe aleatoriamente uma porcentagem das samples da Design Matrix X e do Target Values y, tome muito cuidado, as samples devem ser as mesmas e estarem na mesma ordem, caso contrário, você terá grandes problemas. Vamos pegar apenas 2 samples, separá-las e guardá-las. No nosso exemplo, pegamos as samples com índice 1 e 10. Com elas, criamos um Design Matrix X de Teste e um Target Values y de Teste. Veja a imagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceba que os índices são os mesmos, tanto para X de Teste quanto para y de teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O que sobrou será nossa Design Matrix X de Treino e Target Values y de Treino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![DATASET](images/treino.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos treinar nosso algoritmo no X de treino e y de treino e testar para ver como ele irá se sair em dados que ele nunca viu em X de teste e y de teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation \n",
    "Uma técnica mais avançacada e largamente usada pela comunidade é o cross-validation. Veja só, nós separamos aleatoriamente duas samples do dataset, treinamos no X de treino e y de treino e avaliamos no X de teste e y de teste, mas nada nos garante que, por força do acaso, nós tenhamos separados duas samples que são extremamente fáceis de classificar e podemos superestimar nosso modelo por isso. A maneira de tentar diminuir isso é usando o cross-validation. É como se fizéssemos vários train test split. Vamos para o passo a passo\n",
    "\n",
    "- Já temos o dataset separado em Design Matrix X e Target Values y\n",
    "- Já temos a separação em treino e teste\n",
    "- Faremos o cross-validation nos dados de treino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, como funciona o cross-validation? Primeiro, você escolhe o número de *folders* que você deseja, na comunidade científica geralmente esse numero é 10, sinceramente não sei se há algum critério técnico por trás disso. Aliás, como você irá perceber, não há muitos critérios técnicos e objetivos por trás de muita coisa.<br>\n",
    "\n",
    "No nosso exemplo, iremos usar 5 **folders** apenas por simplicidade. Como temos 10 samples nos dados de treino e 10 dividido por 5 é igual a 2. Na prática, o que acontece é isso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRIMEIRA ITERAÇÃO de 5 (1 de 5)\n",
    "![DATASET](images/cross-fold-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGUNDA ITERAÇÃO DE 5 (2 de 5)\n",
    "![DATASET](images/cross-fold-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TERCEIRA ITERAÇÃO DE 5 (3 de 5)\n",
    "![DATASET](images/cross-fold-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUARTA ITERAÇÃO DE 5 (4 de 5)\n",
    "![DATASET](images/cross-fold-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUINTA ITERAÇÃO DE 5 (5 de 5)\n",
    "![DATASET](images/cross-fold-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada iteração tem um resultado, aqui no nosso caso, teremos cinco. O que nós reportamos ou usamos para escolher um algoritmo e não outro, é a média e o desvio padrão das métricas dessas iterações."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fluxograma do Baseline Model\n",
    "Geralmente nós começamos com uma POC(proof of concept) para a gente mostrar que o desafio é pelo menos encarável e que podemos escalar ou ir melhorando com outras iterações. O que iremos fazer nesse *Baseline* está resumido abaixo:\n",
    "- Leitura dos dois datasets, features.csv e results.csv\n",
    "- Merge dos dois pelo PatientID\n",
    "- Split entre features e targets\n",
    "- Train Test Split\n",
    "- Instanciando e treinando estimador\n",
    "- Cross-validation 10 folds\n",
    "- Avaliando nos dados de teste\n",
    "- Treinando nos dados completos\n",
    "- Gerar tag para versionar o modelo\n",
    "- Model Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script para o Baseline\n",
    "model completo\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datetime import datetime\n",
    "from joblib import dump\n",
    "\n",
    "# constantes\n",
    "PATH_FEATURES = 'data/features.csv'\n",
    "PATH_TARGETS = 'data/result.csv'\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_SEED = 42\n",
    "N_SPLITS = 5\n",
    "SCORING = 'accuracy'\n",
    "\n",
    "df_features = pd.read_csv(PATH_FEATURES)\n",
    "df_targets = pd.read_csv(PATH_TARGETS)\n",
    "\n",
    "# validando id duplicado\n",
    "quant_dup = df_features.loc[:, 'PatientID'].duplicated().sum() + df_targets.loc[:, 'PatientID'].duplicated().sum()\n",
    "print(f'Quant dup ID: {quant_dup}')\n",
    "\n",
    "# merge dos dois datasets pela coluna PatientID\n",
    "df_diabetes = pd.merge(left=df_features, right=df_targets, how='inner', on='PatientID', indicator=True, validate='1:1')\n",
    "df_diabetes = df_diabetes.set_index(keys='PatientID').sort_index(ascending=True)\n",
    "\n",
    "print('\\nDataset completo')\n",
    "print(df_diabetes)\n",
    "\n",
    "print('\\n\\n--------------------------------------------------------------------')\n",
    "print('Infos dataframe')\n",
    "print(df_diabetes.info())\n",
    "\n",
    "# split entre features e labels\n",
    "X = df_diabetes.drop(columns=['Outcome', '_merge'])\n",
    "y = df_diabetes.loc[:, 'Outcome']\n",
    "\n",
    "# separando em dados de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED)\n",
    "\n",
    "# instanciando\n",
    "kfold = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_SEED)\n",
    "grad_boost = GradientBoostingClassifier(random_state=RANDOM_SEED, verbose=0)\n",
    "\n",
    "scores = cross_val_score(grad_boost, X_train, y_train, scoring=SCORING, cv=kfold)\n",
    "\n",
    "# treinando apenas nos dados de treino para prever nos dados de teste\n",
    "grad_boost.fit(X_train, y_train)\n",
    "y_pred = grad_boost.predict(X_test)\n",
    "\n",
    "print('\\n\\n--------------------------------------------------------------------')\n",
    "print(f'Resultados cross-validation {N_SPLITS} splits')\n",
    "print(f'Media   : {scores.mean()*100:.2f}% ')\n",
    "print(f'Desv Pad: {scores.std()*100:.2f}% ')\n",
    "print('-----------------------------------')\n",
    "print(f'Acc teste: {accuracy_score(y_test, y_pred)*100:.2f}%')\n",
    "\n",
    "# treinando nos dados completos\n",
    "grad_boost.fit(X, y)\n",
    "\n",
    "# gerando loggin para versionamento do modelo\n",
    "actual_datetime = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "file_name = actual_datetime + '_grad_boost.joblib'\n",
    "\n",
    "# model persistence\n",
    "dump(grad_boost, file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
